name: ppo-baseline

agent:
  state:
    wrapper:
      name: Simple
    features:
      distance_to_center: True
      speed: True
      steering: True
      heading_errors: [20, continuous]
      neighbor: 8
      ego_pos: True
  action:
    type: 8  # 0: continuous, 1: discrete, 3: ContWithSpeed, 8: AnchorPoint, 5: Trajectory

interface:
  max_episode_steps: 10000
  neighborhood_vehicles:
    radius: 80 #50
  waypoints:
    use_anchor: True
    # number of points in trajectory planning
    lookahead: 30 # larger than size of heading errors 20

policy:
  framework: rllib
  trainer:
    path: ray.rllib.agents.ppo
    name: PPOTrainer

run:
  checkpoint_freq: 5
  checkpoint_at_end: True
  max_failures: 100000
  resume: False
  export_formats: [model, checkpoint]
  stop:
#    time_total_s: 1800
#    time_total_s: 43200
#    time_total_s: 5000
#    time_total_s: 36000
#    time_total_s: 5400
#    time_total_s: 1800
    time_total_s: 10
  config:
    observation_filter: "MeanStdFilter"
#    monitor: True
    log_level: DEBUG
#    log_level: INFO
    num_workers: 4
    num_gpus: 0
    horizon: 1000
    clip_param: 0.2
    num_sgd_iter: 25
#    num_sgd_iter: 25
#    num_sgd_iter: 5
#    sgd_minibatch_size: 64
    sgd_minibatch_size: 32
#    rollout_fragment_length: 200
#    rollout_fragment_length: 400
#    train_batch_size: 400
#    train_batch_size: 1000

#    lr: 1e-3
#    lr: 5e-4
    lr: 1e-4
    lambda: 0
